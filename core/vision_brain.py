import cv2
import json
import time
import numpy as np
import pyrealsense2 as rs
from google import genai
from google.genai import types

class VisionBrain:
    def __init__(self, api_key):
        print("ğŸ‘ï¸ [Vision] Gemini Robotics (Unified Monitor) ë¡œë”© ì¤‘...")
        self.client = genai.Client(api_key=api_key)
        
        self.OFFSET_X = -31
        self.OFFSET_Y = 11
        
        self.pipeline = rs.pipeline()
        self.config = rs.config()
        self.config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
        self.config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
        
        try:
            self.pipeline.start(self.config)
            print("âœ… [Vision] RealSense ì—°ê²° ì„±ê³µ")
        except Exception as e:
            print(f"âŒ [Vision] ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨: {e}")
            self.pipeline = None

    def capture_and_detect(self, target_name):
        if not self.pipeline: return None

        # 1. í”„ë ˆì„ íšë“
        frames = self.pipeline.wait_for_frames()
        depth_frame = frames.get_depth_frame()
        color_frame = frames.get_color_frame()
        
        if not depth_frame or not color_frame: return None

        # 2. ì´ë¯¸ì§€ ë³€í™˜
        depth_image = np.asanyarray(depth_frame.get_data())
        color_image = np.asanyarray(color_frame.get_data())
        
        # Depth ì´ë¯¸ì§€ë¥¼ ì»¬ëŸ¬ë§µìœ¼ë¡œ ë³€í™˜ (ë³´ê¸° ì¢‹ê²Œ)
        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

        # â˜… [í•µì‹¬] ë‘ ì´ë¯¸ì§€ë¥¼ ê°€ë¡œë¡œ í•©ì¹˜ê¸° (Horizontal Stack)
        # ì™¼ìª½: ì»¬ëŸ¬ / ì˜¤ë¥¸ìª½: ëìŠ¤
        combined_image = np.hstack((color_image, depth_colormap))
        
        # ëª¨ë‹ˆí„°ë§ ì°½ ë„ìš°ê¸° (ë¶„ì„ ì „ ê¹¨ë—í•œ í™”ë©´)
        cv2.imshow('HeroBot Monitor', combined_image)
        cv2.waitKey(1)

        h, w, _ = color_image.shape

        # 3. Gemini ì •ë°€ ë¶„ì„ ìš”ì²­
        print(f"   ğŸ§  [Vision] '{target_name}' ì •ë°€ ë¶„ì„ ì¤‘...", end="", flush=True)
        try:
            _, img_bytes = cv2.imencode('.jpg', color_image)
            
            MODEL_NAME = "gemini-robotics-er-1.5-preview"
            
            # ì—„ê²©í•œ í”„ë¡¬í”„íŠ¸ ìœ ì§€
            prompt = f"""
            Find the '{target_name}' in the image.
            
            [Strict Rules]
            1. You must be highly confident. Do NOT mistake paper, shadows, or similar looking objects for the target.
            2. If you are not sure, return [].
            3. If found, return JSON: [{{"point": [y, x], "label": "{target_name}", "confidence": 0.0-1.0}}]
            
            The points are normalized [0-1000].
            """
            
            response = self.client.models.generate_content(
                model=MODEL_NAME,
                contents=[
                    types.Part.from_bytes(data=img_bytes.tobytes(), mime_type='image/jpeg'),
                    prompt
                ],
                config=types.GenerateContentConfig(temperature=0.0)
            )
            
            text = response.text.replace("```json", "").replace("```", "").strip()
            results = json.loads(text)
            
            if not results:
                print(" âŒ ì—†ìŒ (AI íŒë‹¨)")
                return None
            
            target = results[0]
            confidence = target.get('confidence', 0.8) 
            
            # ì‹ ë¢°ë„ ì»· (70% ë¯¸ë§Œ ë¬´ì‹œ)
            if confidence < 0.7:
                print(f" âš ï¸ ì˜ì‹¬ë¨ ({confidence*100:.0f}%) -> ë¬´ì‹œ")
                return None

            # 4. ì¢Œí‘œ ê³„ì‚° ë° ê±°ë¦¬ ì¸¡ì •
            norm_y, norm_x = target['point']
            cam_x = int((norm_x / 1000.0) * w)
            cam_y = int((norm_y / 1000.0) * h)
            
            depth_x = max(0, min(cam_x + self.OFFSET_X, 639))
            depth_y = max(0, min(cam_y + self.OFFSET_Y, 479))
            dist = depth_frame.get_distance(depth_x, depth_y)
            
            # ê±°ë¦¬ ì˜ˆì™¸ ì²˜ë¦¬
            if dist == 0 or dist > 2.0:
                 print(f" âš ï¸ ê±°ë¦¬ ì˜¤ë¥˜ ({dist:.2f}m) -> ë¬´ì‹œ")
                 return None

            print(f" âœ¨ í™•ì •! ({confidence*100:.0f}%, {dist:.2f}m)")
            
            # 5. ê²°ê³¼ ê·¸ë¦¬ê¸° (ì°¾ì•˜ì„ ë•Œë§Œ)
            # ì»¬ëŸ¬ ì´ë¯¸ì§€ì— íƒ€ê²Ÿ í‘œì‹œ
            cv2.circle(color_image, (cam_x, cam_y), 10, (0, 255, 0), -1)
            cv2.putText(color_image, f"{target_name} ({confidence*100:.0f}%)", (cam_x + 10, cam_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)
            
            # ëìŠ¤ ì´ë¯¸ì§€ì—ë„ íƒ€ê²Ÿ í‘œì‹œ (ìœ„ì¹˜ í™•ì¸ìš©)
            cv2.circle(depth_colormap, (cam_x, cam_y), 10, (0, 255, 255), -1)
            
            # â˜… ë‹¤ì‹œ í•©ì³ì„œ ì—…ë°ì´íŠ¸ëœ í™”ë©´ ë³´ì—¬ì£¼ê¸°
            combined_result = np.hstack((color_image, depth_colormap))
            cv2.imshow('HeroBot Monitor', combined_result)
            cv2.waitKey(1)
            
            return {"found": True, "x": cam_x, "y": cam_y, "dist": dist}

        except Exception as e:
            print(f" âš ï¸ ë¶„ì„ ì—ëŸ¬: {e}")
            return None

    def close(self):
        if self.pipeline: self.pipeline.stop()
        cv2.destroyAllWindows()